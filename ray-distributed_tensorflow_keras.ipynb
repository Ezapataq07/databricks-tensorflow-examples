{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f918fedc-8134-4621-92eb-bbb64b400a84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Get Started with Distributed Training using TensorFlow/Keras\n",
    "Ray Trainâ€™s TensorFlow integration enables you to scale your TensorFlow and Keras training functions to many machines and GPUs by configuring `TF_CONFIG` and managing worker processes for you.\n",
    "\n",
    "On a technical level, Ray Train schedules your training workers and configures `TF_CONFIG` for you, allowing you to run your `MultiWorkerMirroredStrategy` training script. See Distributed training with TensorFlow for more information.\n",
    "\n",
    "Most of the examples in this guide use TensorFlow with Keras, but Ray Train also works with vanilla TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4b43a8f-0856-4c2f-9e6a-9fa58b1e4afe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c45f891a-df8e-4ac2-b27e-d748bcc4d7c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import tensorflow as tf\n",
    "\n",
    "from ray import train\n",
    "from ray.train import ScalingConfig\n",
    "from ray.train.tensorflow import TensorflowTrainer\n",
    "from ray.train.tensorflow.keras import ReportCheckpointCallback\n",
    "\n",
    "\n",
    "# If using GPUs, set this to True.\n",
    "use_gpu = False\n",
    "\n",
    "a = 5\n",
    "b = 10\n",
    "size = 100\n",
    "\n",
    "\n",
    "def build_model() -> tf.keras.Model:\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(input_shape=()),\n",
    "            # Add feature dimension, expanding (batch_size,) to (batch_size, 1).\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10),\n",
    "            tf.keras.layers.Dense(1),\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_func(config: dict):\n",
    "    batch_size = config.get(\"batch_size\", 64)\n",
    "    epochs = config.get(\"epochs\", 3)\n",
    "\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        # Model building/compiling need to be within `strategy.scope()`.\n",
    "        multi_worker_model = build_model()\n",
    "        multi_worker_model.compile(\n",
    "            optimizer=tf.keras.optimizers.SGD(learning_rate=config.get(\"lr\", 1e-3)),\n",
    "            loss=tf.keras.losses.mean_squared_error,\n",
    "            metrics=[tf.keras.metrics.mean_squared_error],\n",
    "        )\n",
    "\n",
    "    dataset = train.get_dataset_shard(\"train\")\n",
    "\n",
    "    results = []\n",
    "    for _ in range(epochs):\n",
    "        tf_dataset = dataset.to_tf(\n",
    "            feature_columns=\"x\", label_columns=\"y\", batch_size=batch_size\n",
    "        )\n",
    "        history = multi_worker_model.fit(\n",
    "            tf_dataset, callbacks=[ReportCheckpointCallback()]\n",
    "        )\n",
    "        results.append(history.history)\n",
    "    return results\n",
    "\n",
    "\n",
    "config = {\"lr\": 1e-3, \"batch_size\": 32, \"epochs\": 4}\n",
    "\n",
    "train_dataset = ray.data.from_items(\n",
    "    [{\"x\": x / 200, \"y\": 2 * x / 200} for x in range(200)]\n",
    ")\n",
    "scaling_config = ScalingConfig(num_workers=1, use_gpu=use_gpu)\n",
    "trainer = TensorflowTrainer(\n",
    "    train_loop_per_worker=train_func,\n",
    "    train_loop_config=config,\n",
    "    scaling_config=scaling_config,\n",
    "    datasets={\"train\": train_dataset},\n",
    ")\n",
    "result = trainer.fit()\n",
    "print(result.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afe18749-958b-452b-bbfc-b774985a3eef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Update your training function\n",
    "Wrap your model building and compilation in a `MultiWorkerMirroredStrategy` scope:\n",
    "```python\n",
    "with tf.distribute.MultiWorkerMirroredStrategy().scope():\n",
    "    model = build_model()\n",
    "    model.compile(...)\n",
    "```\n",
    "Adjust your batch size to global batch size:\n",
    "```diff\n",
    "- batch_size = worker_batch_size\n",
    "+ batch_size = worker_batch_size * train.get_context().get_world_size()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "779e651c-3bdd-49a3-9752-57b7dbc826ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create a TensorflowTrainer\n",
    "Instantiate a `TensorflowTrainer` with the desired scaling configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b12fceb-a055-4c1f-96f1-d6f9822724d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ray.train import ScalingConfig\n",
    "from ray.train.tensorflow import TensorflowTrainer, TensorflowConfig\n",
    "\n",
    "# For GPU training, set use_gpu=True\n",
    "trainer = TensorflowTrainer(\n",
    "    train_loop_per_worker=train_func,\n",
    "    scaling_config=ScalingConfig(use_gpu=False, num_workers=1),\n",
    "    tensorflow_backend=TensorflowConfig(),  # optional custom backend config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4adf7b0-b432-4d54-877c-6984e4c46893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run a training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5e48633-ca89-4051-a134-8191f6720bf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3544906-9b27-4c9d-8d6e-b9a2957f1cb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load and preprocess data\n",
    "Convert a Ray Dataset shard into a TensorFlow dataset:\n",
    "```python\n",
    "from ray import train\n",
    "from ray.train.tensorflow import prepare_dataset_shard\n",
    "\n",
    "def train_func(config: dict):\n",
    "    dataset_shard = train.get_context().get_dataset_shard('train')\n",
    "    def to_tf_dataset(ds, batch_size):\n",
    "        tf_ds = ds.to_tf(feature_columns='image', label_columns='label', batch_size=batch_size)\n",
    "        return prepare_dataset_shard(tf_ds)\n",
    "    # ... use tf_ds ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "924d5ea3-4220-414c-b764-caa33a831606",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Report results\n",
    "Use `ReportCheckpointCallback` to automatically report metrics and checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e349b11-e92c-4026-91e0-41b17d7f4fda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ray.train.tensorflow.keras import ReportCheckpointCallback\n",
    "\n",
    "def train_func(config: dict):\n",
    "    # ...\n",
    "    for epoch in range(config['epochs']):\n",
    "        history = model.fit(dataset, callbacks=[ReportCheckpointCallback()])\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc9b5687-edb3-4001-ba53-63a6c417d19b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save and load checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bad13ab6-c215-4c55-9143-368f10159c0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from ray import train\n",
    "from ray.train import Checkpoint, ScalingConfig\n",
    "from ray.train.tensorflow import TensorflowTrainer\n",
    "\n",
    "def train_func(config):\n",
    "    # ...\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        # Save model and epoch metadata\n",
    "        with tempfile.TemporaryDirectory() as tmp:\n",
    "            model.save(f\"{tmp}/model.keras\")\n",
    "            checkpoint = Checkpoint.from_directory(tmp)\n",
    "            train.report({'loss': history.history['loss'][0]}, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "180331d2-d580-4683-812d-cf23a71d9f5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading from checkpoint\n",
    "def train_func(config):\n",
    "    checkpoint = train.get_checkpoint()\n",
    "    if checkpoint:\n",
    "        with checkpoint.as_directory() as d:\n",
    "            model = tf.keras.models.load_model(f\"{d}/model.keras\")\n",
    "    else:\n",
    "        model = build_model()\n",
    "    model.compile(...)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "665ce573-82e1-40db-a619-b0358e4ccb34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Further reading\n",
    "- [Experiment tracking]\n",
    "- [Fault tolerance and spot instances]\n",
    "- [Hyperparameter optimization]"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ray-distributed_tensorflow_keras",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
